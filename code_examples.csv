filename,purpose,key_features,code_snippet
retry_handler.py,Implements exponential backoff for rate limiting and errors,"HTTP 429/5xx handling, configurable retry limits, jitter","
import time
import random
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

class RetryHandler:
    def __init__(self, max_retries=5, backoff_factor=2):
        self.max_retries = max_retries
        self.backoff_factor = backoff_factor

    def create_session(self):
        session = requests.Session()
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=self.backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=[""HEAD"", ""GET"", ""OPTIONS""]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount(""https://"", adapter)
        session.mount(""http://"", adapter)
        return session

    def exponential_backoff_with_jitter(self, attempt):
        wait_time = min(self.backoff_factor ** attempt, 60)
        jitter = random.uniform(0, 0.1 * wait_time)
        return wait_time + jitter
"
jira_client.py,Handles Jira REST API interactions with pagination,"Pagination management, authentication, rate limiting","
import requests
import os

class JiraClient:
    def __init__(self, base_url=""https://issues.apache.org/jira""):
        self.base_url = base_url
        self.session = self._create_session()

    def fetch_issues(self, project_key, start_at=0, max_results=100):
        url = f""{self.base_url}/rest/api/2/search""
        params = {
            ""jql"": f""project={project_key}"",
            ""startAt"": start_at,
            ""maxResults"": max_results,
            ""fields"": ""summary,description,status,priority,assignee,created,updated,labels""
        }
        response = self.session.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()

    def fetch_comments(self, issue_key):
        url = f""{self.base_url}/rest/api/2/issue/{issue_key}/comment""
        response = self.session.get(url, timeout=30)
        response.raise_for_status()
        return response.json()

    def get_all_issues(self, project_key, batch_size=100):
        start_at = 0
        while True:
            batch = self.fetch_issues(project_key, start_at, batch_size)
            issues = batch.get('issues', [])
            if not issues:
                break
            yield issues
            start_at += batch_size
            if start_at >= batch['total']:
                break
"
checkpoint_manager.py,Manages state persistence for fault tolerance,"Save/load state, atomic writes, resume capability","
import json
import os
from pathlib import Path

class CheckpointManager:
    def __init__(self, checkpoint_dir='checkpoints'):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)

    def save_checkpoint(self, project_key, state):
        checkpoint_file = self.checkpoint_dir / f""{project_key}.json""
        temp_file = checkpoint_file.with_suffix('.tmp')

        # Atomic write: write to temp, then rename
        with open(temp_file, 'w') as f:
            json.dump(state, f, indent=2)
        temp_file.replace(checkpoint_file)

    def load_checkpoint(self, project_key):
        checkpoint_file = self.checkpoint_dir / f""{project_key}.json""
        if checkpoint_file.exists():
            with open(checkpoint_file, 'r') as f:
                return json.load(f)
        return None

    def get_state(self, project_key):
        state = self.load_checkpoint(project_key)
        if state is None:
            return {
                'project_key': project_key,
                'last_processed_index': 0,
                'total_issues_processed': 0,
                'timestamp': None
            }
        return state
"
jsonl_writer.py,Writes transformed data to JSONL format,"Buffered writing, proper encoding, validation","
import json
from pathlib import Path

class JSONLWriter:
    def __init__(self, output_file, buffer_size=100):
        self.output_file = Path(output_file)
        self.buffer = []
        self.buffer_size = buffer_size

    def write_record(self, record):
        self.buffer.append(record)
        if len(self.buffer) >= self.buffer_size:
            self.flush()

    def flush(self):
        if not self.buffer:
            return

        with open(self.output_file, 'a', encoding='utf-8') as f:
            for record in self.buffer:
                json_line = json.dumps(record, ensure_ascii=False)
                f.write(json_line + '\n')

        self.buffer.clear()

    def close(self):
        self.flush()
"
transformer.py,Transforms Jira data to LLM training format,"Text cleaning, task generation, JSONL formatting","
import re
from html import unescape

class IssueTransformer:
    def __init__(self):
        pass

    def clean_text(self, text):
        if not text:
            return """"
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        # Decode HTML entities
        text = unescape(text)
        # Normalize whitespace
        text = ' '.join(text.split())
        return text

    def transform_to_jsonl(self, issue, comments):
        issue_data = {
            ""metadata"": {
                ""issue_key"": issue.get('key'),
                ""project"": issue['fields'].get('project', {}).get('key'),
                ""status"": issue['fields'].get('status', {}).get('name'),
                ""priority"": issue['fields'].get('priority', {}).get('name'),
                ""created"": issue['fields'].get('created'),
                ""updated"": issue['fields'].get('updated'),
                ""labels"": issue['fields'].get('labels', [])
            },
            ""title"": self.clean_text(issue['fields'].get('summary', '')),
            ""description"": self.clean_text(issue['fields'].get('description', '')),
            ""comments"": [
                {
                    ""author"": c.get('author', {}).get('displayName'),
                    ""body"": self.clean_text(c.get('body', '')),
                    ""created"": c.get('created')
                }
                for c in comments.get('comments', [])
            ]
        }

        # Generate training tasks
        tasks = self.generate_tasks(issue_data)
        return {**issue_data, ""tasks"": tasks}

    def generate_tasks(self, issue_data):
        tasks = []

        # Summarization task
        if issue_data['description']:
            tasks.append({
                ""task_type"": ""summarization"",
                ""input"": issue_data['description'],
                ""target"": issue_data['title']
            })

        # Classification task
        tasks.append({
            ""task_type"": ""classification"",
            ""input"": f""{issue_data['title']} {issue_data['description']}"",
            ""target"": {
                ""status"": issue_data['metadata']['status'],
                ""priority"": issue_data['metadata']['priority']
            }
        })

        return tasks
"
